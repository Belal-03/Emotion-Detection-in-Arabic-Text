{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a6529c-0599-4134-ae6a-1beba0517720",
   "metadata": {},
   "source": [
    "# <center> • **NLP and Text Mining** </center>\n",
    "### <center> _____________• Assignment 04: Emotion Detection in Arabic Text Using Classical and Deep Learning Techniques_____________ </center>\n",
    "### <center> _____________• By:_____________ </center>\n",
    "#### <center> _____________○ Belal Khaled ~ 2136873_____________ </center>\n",
    "#### <center> _____________○ Yaseen Naser ~ 2130397_____________ </center>\n",
    "#### <center> _____________○ Abdulrahman Abuhani ~ 2132462_____________ </center>\n",
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f1b76-33e7-4f99-9cbc-1f865e29e75f",
   "metadata": {},
   "source": [
    "# <center> • EmotionalTone Arabic Dataset: </center>\n",
    "## <center> ○ ____________https://github.com/amrmalkhatib/Emotional-Tone/tree/master____________</center>\n",
    "## <center> ○ ____________The best results over a set of eight emotions were obtained using a complement Naïve Bayes algorithm with an overall accuracy of 68.12 %.____________</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e1a1b-b4ea-448f-9ad3-b87c0a945335",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d00f3-4a5b-4cef-be76-05b3463fc2b1",
   "metadata": {},
   "source": [
    "# • Importing All Libraries we Need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7242c099-f191-4fd3-8e26-35ad012f470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acbfe9-032d-4c79-8659-c9049998ba5c",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6977b-528c-4fd7-b50e-18af66b335f7",
   "metadata": {},
   "source": [
    "# • Load the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08281e5a-b39a-494a-b0bd-0f8c4181fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Emotional-Tone-Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb83b477-80d0-4cf0-a329-2437a2578f7a",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c23e0-f50c-47d2-9ed4-2e1b52569767",
   "metadata": {},
   "source": [
    "# • Explore the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0501f695-d487-47c4-8c3f-21f3b026dffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TWEET</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>الاوليمبياد الجايه هكون لسه ف الكليه ..</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>عجز الموازنه وصل ل93.7 % من الناتج المحلي يعني...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>كتنا نيله ف حظنا الهباب xD</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>جميعنا نريد تحقيق اهدافنا لكن تونس تالقت في حر...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>الاوليمبياد نظامها مختلف .. ومواعيد المونديال ...</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TWEET    LABEL\n",
       "0   1            الاوليمبياد الجايه هكون لسه ف الكليه ..     none\n",
       "1   2  عجز الموازنه وصل ل93.7 % من الناتج المحلي يعني...    anger\n",
       "2   3                         كتنا نيله ف حظنا الهباب xD  sadness\n",
       "3   4  جميعنا نريد تحقيق اهدافنا لكن تونس تالقت في حر...      joy\n",
       "4   5  الاوليمبياد نظامها مختلف .. ومواعيد المونديال ...     none"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819e89e0-387e-4d58-8c13-324882e93caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10065 entries, 0 to 10064\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      10065 non-null  int64 \n",
      " 1    TWEET  10064 non-null  object\n",
      " 2    LABEL  10065 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 236.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9411c388-2772-4818-9e93-f6d3e5261f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10065.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5033.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2905.659564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2517.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5033.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10065.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID\n",
       "count  10065.000000\n",
       "mean    5033.000000\n",
       "std     2905.659564\n",
       "min        1.000000\n",
       "25%     2517.000000\n",
       "50%     5033.000000\n",
       "75%     7549.000000\n",
       "max    10065.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9cb6ca7-eea6-4f7d-8ccc-3fd1cc690a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', ' TWEET', ' LABEL'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a385f0ba-dd3b-4326-8578-0111f16b9592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'TWEET', 'LABEL'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = data.columns.str.strip()\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a3ebf-2fb9-4a92-8883-e7c323ee1731",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0a069-a106-4d06-b4b2-f6b964e112ec",
   "metadata": {},
   "source": [
    "# • Clean the Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "945006b2-44c9-4429-89ad-16a1737297b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>cleaned_TWEET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الاوليمبياد الجايه هكون لسه ف الكليه ..</td>\n",
       "      <td>الاوليمبياد الجايه هكون لسه الكليه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>عجز الموازنه وصل ل93.7 % من الناتج المحلي يعني...</td>\n",
       "      <td>عجز الموازنه وصل الناتج المحلي يعني لسه اقل نف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>كتنا نيله ف حظنا الهباب xD</td>\n",
       "      <td>كتنا نيله حظنا الهباب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>جميعنا نريد تحقيق اهدافنا لكن تونس تالقت في حر...</td>\n",
       "      <td>جميعنا نريد تحقيق اهدافنا تونس تالقت حراسه المرمي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الاوليمبياد نظامها مختلف .. ومواعيد المونديال ...</td>\n",
       "      <td>الاوليمبياد نظامها مختلف ومواعيد المونديال مكا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TWEET  \\\n",
       "0            الاوليمبياد الجايه هكون لسه ف الكليه ..   \n",
       "1  عجز الموازنه وصل ل93.7 % من الناتج المحلي يعني...   \n",
       "2                         كتنا نيله ف حظنا الهباب xD   \n",
       "3  جميعنا نريد تحقيق اهدافنا لكن تونس تالقت في حر...   \n",
       "4  الاوليمبياد نظامها مختلف .. ومواعيد المونديال ...   \n",
       "\n",
       "                                       cleaned_TWEET  \n",
       "0                 الاوليمبياد الجايه هكون لسه الكليه  \n",
       "1  عجز الموازنه وصل الناتج المحلي يعني لسه اقل نف...  \n",
       "2                              كتنا نيله حظنا الهباب  \n",
       "3  جميعنا نريد تحقيق اهدافنا تونس تالقت حراسه المرمي  \n",
       "4  الاوليمبياد نظامها مختلف ومواعيد المونديال مكا...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# • Remove Unwanted Characters Like Punctuation, Numbers, and Non-Arabic Letters.\n",
    "# • Remove Diacritics (Short Vowels) to Normalize Arabic Text.\n",
    "# • Remove Stop Words (Common Words Like \"و\", \"في\", \"على\" That Do Not Contribute to Emotions).\n",
    "\n",
    "#Function to Clean Arabic Text:\n",
    "def clean_text(text):\n",
    "    #Check If Text is Not a String:\n",
    "    if not isinstance(text, str):  \n",
    "        return \"\"\n",
    "    #Removing Punctuation and Numbers:\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)  \n",
    "    #Removing Diacritics:\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
    "    #Removing Stop Words:\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    text = ' '.join([word for word in text.split() if word not in arabic_stopwords])\n",
    "    return text\n",
    "\n",
    "#Applying the clean_text Function to the 'TWEET' Column (Adding New Column 'cleaned_TWEET'):\n",
    "data['cleaned_TWEET'] = data['TWEET'].apply(clean_text)\n",
    "\n",
    "#Displaying 'TWEET' Column and 'cleaned_TWEET' Column:\n",
    "data[[\"TWEET\", \"cleaned_TWEET\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872a2d7-71cf-4298-ade8-ef8daf6ad62d",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b8a0d-788c-42c5-b941-e7121c240f5d",
   "metadata": {},
   "source": [
    "# • Tokenize the Arabic Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee147f72-8f0f-40ea-9835-feed6c197754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_TWEET</th>\n",
       "      <th>tokenized_TWEET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الاوليمبياد الجايه هكون لسه الكليه</td>\n",
       "      <td>[الاوليمبياد, الجايه, هكون, لسه, الكليه]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>عجز الموازنه وصل الناتج المحلي يعني لسه اقل نف...</td>\n",
       "      <td>[عجز, الموازنه, وصل, الناتج, المحلي, يعني, لسه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>كتنا نيله حظنا الهباب</td>\n",
       "      <td>[كتنا, نيله, حظنا, الهباب]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>جميعنا نريد تحقيق اهدافنا تونس تالقت حراسه المرمي</td>\n",
       "      <td>[جميعنا, نريد, تحقيق, اهدافنا, تونس, تالقت, حر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الاوليمبياد نظامها مختلف ومواعيد المونديال مكا...</td>\n",
       "      <td>[الاوليمبياد, نظامها, مختلف, ومواعيد, المونديا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       cleaned_TWEET  \\\n",
       "0                 الاوليمبياد الجايه هكون لسه الكليه   \n",
       "1  عجز الموازنه وصل الناتج المحلي يعني لسه اقل نف...   \n",
       "2                              كتنا نيله حظنا الهباب   \n",
       "3  جميعنا نريد تحقيق اهدافنا تونس تالقت حراسه المرمي   \n",
       "4  الاوليمبياد نظامها مختلف ومواعيد المونديال مكا...   \n",
       "\n",
       "                                     tokenized_TWEET  \n",
       "0           [الاوليمبياد, الجايه, هكون, لسه, الكليه]  \n",
       "1  [عجز, الموازنه, وصل, الناتج, المحلي, يعني, لسه...  \n",
       "2                         [كتنا, نيله, حظنا, الهباب]  \n",
       "3  [جميعنا, نريد, تحقيق, اهدافنا, تونس, تالقت, حر...  \n",
       "4  [الاوليمبياد, نظامها, مختلف, ومواعيد, المونديا...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the 'cleaned_TWEET' Column (Adding New Column 'tokenized_TWEET'):\n",
    "data['tokenized_TWEET'] = data['cleaned_TWEET'].apply(word_tokenize)\n",
    "\n",
    "#Displaying 'cleaned_TWEET' Column and 'tokenized_TWEET' Column:\n",
    "data[[\"cleaned_TWEET\", \"tokenized_TWEET\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6414b-1aa4-477e-a79e-f1c4fb22177d",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8a609-5498-4d54-a647-22439515350c",
   "metadata": {},
   "source": [
    "# • Normalize the Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aaf84b4-20b4-4bef-a1ca-2a8c1bfb0ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_TWEET</th>\n",
       "      <th>normalized_TWEET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[الاوليمبياد, الجايه, هكون, لسه, الكليه]</td>\n",
       "      <td>[الاوليمبياد, الجايه, هكون, لسه, الكليه]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[عجز, الموازنه, وصل, الناتج, المحلي, يعني, لسه...</td>\n",
       "      <td>[عجز, الموازنه, وصل, الناتج, المحلي, يعني, لسه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[كتنا, نيله, حظنا, الهباب]</td>\n",
       "      <td>[كتنا, نيله, حظنا, الهباب]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[جميعنا, نريد, تحقيق, اهدافنا, تونس, تالقت, حر...</td>\n",
       "      <td>[جميعنا, نريد, تحقيق, اهدافنا, تونس, تالقت, حر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[الاوليمبياد, نظامها, مختلف, ومواعيد, المونديا...</td>\n",
       "      <td>[الاوليمبياد, نظامها, مختلف, ومواعيد, المونديا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     tokenized_TWEET  \\\n",
       "0           [الاوليمبياد, الجايه, هكون, لسه, الكليه]   \n",
       "1  [عجز, الموازنه, وصل, الناتج, المحلي, يعني, لسه...   \n",
       "2                         [كتنا, نيله, حظنا, الهباب]   \n",
       "3  [جميعنا, نريد, تحقيق, اهدافنا, تونس, تالقت, حر...   \n",
       "4  [الاوليمبياد, نظامها, مختلف, ومواعيد, المونديا...   \n",
       "\n",
       "                                    normalized_TWEET  \n",
       "0           [الاوليمبياد, الجايه, هكون, لسه, الكليه]  \n",
       "1  [عجز, الموازنه, وصل, الناتج, المحلي, يعني, لسه...  \n",
       "2                         [كتنا, نيله, حظنا, الهباب]  \n",
       "3  [جميعنا, نريد, تحقيق, اهدافنا, تونس, تالقت, حر...  \n",
       "4  [الاوليمبياد, نظامها, مختلف, ومواعيد, المونديا...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# • Handle Word Variants, Such as:\n",
    "    # • Variants of \"أ\", \"إ\", and \"آ\" → Normalize to \"ا\".\n",
    "    # • Variants of \"ة\" → Normalize to \"ه\".\n",
    "    # • Variants of repeated characters.\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)  \n",
    "    text = re.sub(r'ة', 'ه', text)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text) \n",
    "    return text\n",
    "\n",
    "#Applying Normalization to the 'tokenized_TWEET' Column (Adding New Column 'normalized_TWEET'):\n",
    "data['normalized_TWEET'] = data['tokenized_TWEET'].apply(lambda x: [normalize_text(word) for word in x])\n",
    "\n",
    "#Displaying 'tokenized_TWEET' Column and 'normalized_TWEET' Column:\n",
    "data[[\"tokenized_TWEET\", \"normalized_TWEET\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a73c33-2032-4c05-bcbf-bf5ce75af460",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489457cc-ae65-45c1-ad38-94e2ad919b07",
   "metadata": {},
   "source": [
    "# • Bag-of-Words (BoW):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67ef20fb-0f82-422b-9441-369a037fbe9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>اب</th>\n",
       "      <th>ابتديت</th>\n",
       "      <th>ابتسامتك</th>\n",
       "      <th>ابتسامه</th>\n",
       "      <th>ابتسم</th>\n",
       "      <th>ابحث</th>\n",
       "      <th>ابدا</th>\n",
       "      <th>ابداع</th>\n",
       "      <th>ابراهيم</th>\n",
       "      <th>ابرز</th>\n",
       "      <th>...</th>\n",
       "      <th>يوفقه</th>\n",
       "      <th>يوفقهم</th>\n",
       "      <th>يول</th>\n",
       "      <th>يوم</th>\n",
       "      <th>يوما</th>\n",
       "      <th>يومه</th>\n",
       "      <th>يومها</th>\n",
       "      <th>يومين</th>\n",
       "      <th>يونس</th>\n",
       "      <th>گل</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   اب  ابتديت  ابتسامتك  ابتسامه  ابتسم  ابحث  ابدا  ابداع  ابراهيم  ابرز  \\\n",
       "0   0       0         0        0      0     0     0      0        0     0   \n",
       "1   0       0         0        0      0     0     0      0        0     0   \n",
       "2   0       0         0        0      0     0     0      0        0     0   \n",
       "3   0       0         0        0      0     0     0      0        0     0   \n",
       "4   0       0         0        0      0     0     0      0        0     0   \n",
       "\n",
       "   ...  يوفقه  يوفقهم  يول  يوم  يوما  يومه  يومها  يومين  يونس  گل  \n",
       "0  ...      0       0    0    0     0     0      0      0     0   0  \n",
       "1  ...      0       0    0    0     0     0      0      0     0   0  \n",
       "2  ...      0       0    0    0     0     0      0      0     0   0  \n",
       "3  ...      0       0    0    0     0     0      0      0     0   0  \n",
       "4  ...      0       0    0    0     0     0      0      0     0   0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Joining the Tokenized Words Back Into Sentences:\n",
    "data['normalized_TWEET'] = data['normalized_TWEET'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Initializing the CountVectorizer:\n",
    "BOW_vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "#Fitting and Transforming the Text Data:\n",
    "BOW_features = BOW_vectorizer.fit_transform(data['normalized_TWEET'])\n",
    "\n",
    "#Converting to DataFrame for Better Readability:\n",
    "BOW_df = pd.DataFrame(BOW_features.toarray(), columns=BOW_vectorizer.get_feature_names_out())\n",
    "\n",
    "BOW_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68485cf2-dd3d-425d-86c3-cf855e963668",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb03d792-8d0e-4d4f-8a84-9eae66cd0ed5",
   "metadata": {},
   "source": [
    "# • TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0d0226a-3fe3-464e-a464-ca78fc82d1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>اب</th>\n",
       "      <th>ابتديت</th>\n",
       "      <th>ابتسامتك</th>\n",
       "      <th>ابتسامه</th>\n",
       "      <th>ابتسم</th>\n",
       "      <th>ابحث</th>\n",
       "      <th>ابدا</th>\n",
       "      <th>ابداع</th>\n",
       "      <th>ابراهيم</th>\n",
       "      <th>ابرز</th>\n",
       "      <th>...</th>\n",
       "      <th>يوفقه</th>\n",
       "      <th>يوفقهم</th>\n",
       "      <th>يول</th>\n",
       "      <th>يوم</th>\n",
       "      <th>يوما</th>\n",
       "      <th>يومه</th>\n",
       "      <th>يومها</th>\n",
       "      <th>يومين</th>\n",
       "      <th>يونس</th>\n",
       "      <th>گل</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    اب  ابتديت  ابتسامتك  ابتسامه  ابتسم  ابحث  ابدا  ابداع  ابراهيم  ابرز  \\\n",
       "0  0.0     0.0       0.0      0.0    0.0   0.0   0.0    0.0      0.0   0.0   \n",
       "1  0.0     0.0       0.0      0.0    0.0   0.0   0.0    0.0      0.0   0.0   \n",
       "2  0.0     0.0       0.0      0.0    0.0   0.0   0.0    0.0      0.0   0.0   \n",
       "3  0.0     0.0       0.0      0.0    0.0   0.0   0.0    0.0      0.0   0.0   \n",
       "4  0.0     0.0       0.0      0.0    0.0   0.0   0.0    0.0      0.0   0.0   \n",
       "\n",
       "   ...  يوفقه  يوفقهم  يول  يوم  يوما  يومه  يومها  يومين  يونس   گل  \n",
       "0  ...    0.0     0.0  0.0  0.0   0.0   0.0    0.0    0.0   0.0  0.0  \n",
       "1  ...    0.0     0.0  0.0  0.0   0.0   0.0    0.0    0.0   0.0  0.0  \n",
       "2  ...    0.0     0.0  0.0  0.0   0.0   0.0    0.0    0.0   0.0  0.0  \n",
       "3  ...    0.0     0.0  0.0  0.0   0.0   0.0    0.0    0.0   0.0  0.0  \n",
       "4  ...    0.0     0.0  0.0  0.0   0.0   0.0    0.0    0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing the TfidfVectorizer:\n",
    "TFIDF_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "#Fitting and Transforming the Text Data:\n",
    "TFIDF_features = TFIDF_vectorizer.fit_transform(data['normalized_TWEET'])\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "TFIDF_df = pd.DataFrame(TFIDF_features.toarray(), columns=TFIDF_vectorizer.get_feature_names_out())\n",
    "\n",
    "TFIDF_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c0c8c-a608-4f43-a58a-0819b06db0b2",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c7741-9ad3-41d3-b137-7bb57e4042f8",
   "metadata": {},
   "source": [
    "# • Word2Vec (Pre-trained Models like AraVec or fastText):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4237b61-d6da-44f8-80f1-bacd8f270357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-9.59469937e-03,  1.57624990e-01, -2.03452021e-01,  1.17573902e-01,\n",
       "        1.06548741e-01, -9.86032709e-02, -6.60574958e-02,  3.56602371e-02,\n",
       "       -9.03370902e-02,  7.30422288e-02, -1.44537613e-01,  2.68261373e-01,\n",
       "        5.66276796e-02,  3.77053432e-02,  6.25307709e-02,  6.56563193e-02,\n",
       "        2.02920884e-01,  2.83338696e-01,  1.41982645e-01,  2.84037173e-01,\n",
       "       -4.19083744e-01,  4.45718281e-02,  1.37060240e-01, -2.91097467e-03,\n",
       "       -1.24953575e-02,  3.20910923e-02,  1.06452722e-02,  2.42769405e-01,\n",
       "        1.54280469e-01, -1.27435476e-01, -1.04157425e-01,  1.28446892e-01,\n",
       "        1.70553580e-01, -5.80303837e-03,  2.42562100e-01, -1.28271906e-02,\n",
       "        1.57604814e-01, -1.53687134e-01,  4.56495345e-01, -8.34420044e-03,\n",
       "        2.05491967e-02,  1.15231551e-01,  4.25996967e-02, -2.58710589e-02,\n",
       "       -1.07627556e-01, -9.87990573e-02,  1.21960461e-01, -3.84536397e-04,\n",
       "        1.10248491e-01,  9.21784788e-02,  2.91089956e-02,  2.40413785e-01,\n",
       "        8.10044259e-02,  6.80156425e-02,  5.41392388e-03, -3.04839075e-01,\n",
       "       -2.40030885e-01, -3.79416570e-02,  8.53282958e-02, -2.13600487e-01,\n",
       "       -1.35766476e-01, -1.08155444e-01, -3.16930190e-02,  4.93715517e-02,\n",
       "       -8.97521153e-02, -1.00637257e-01, -2.20862478e-01,  5.44167049e-02,\n",
       "       -4.79671347e-04,  5.37967496e-02,  8.90884101e-02,  2.49592513e-01,\n",
       "        1.84567526e-01,  2.87692845e-02,  3.41634274e-01,  7.72554576e-02,\n",
       "        1.28891140e-01,  2.41052777e-01, -4.15517576e-02,  6.25494123e-02,\n",
       "       -1.85463876e-02,  1.97138280e-01, -9.48513970e-02, -1.31227281e-02,\n",
       "       -1.89216599e-01, -1.39030635e-01,  5.66492006e-02, -9.29911211e-02,\n",
       "       -6.82952181e-02,  3.48320633e-01,  8.52513537e-02, -9.17389393e-02,\n",
       "        1.08714782e-01, -2.10748509e-01, -2.02217907e-01, -4.16470841e-02,\n",
       "        1.14398815e-01, -2.08871067e-02, -1.99447438e-01,  7.86841884e-02,\n",
       "       -1.24333881e-01,  2.57603377e-01,  1.65852904e-01, -8.10827166e-02,\n",
       "        3.82111296e-02,  6.71163574e-02, -2.79677600e-01,  8.09236020e-02,\n",
       "        2.08997652e-01,  2.87326910e-02, -1.39246687e-01, -4.93337736e-02,\n",
       "        2.69742068e-02,  9.79162902e-02, -1.52063267e-02, -1.41164929e-01,\n",
       "        2.46829122e-01,  3.87747958e-02,  4.81700860e-02, -3.08904294e-02,\n",
       "        1.81129709e-01,  2.41191953e-01, -1.57647461e-01, -6.15073442e-02,\n",
       "       -1.46952540e-01,  1.45763662e-02,  7.03390688e-02, -3.01081359e-01,\n",
       "        1.07162677e-01,  1.23069689e-01,  1.16620585e-01,  2.06164986e-01,\n",
       "       -2.33981520e-01, -3.64763916e-01, -1.73861738e-02,  1.13101751e-02,\n",
       "        2.14275032e-01, -1.64130181e-02, -1.99321344e-01,  1.78043529e-01,\n",
       "       -2.03133494e-01,  3.53156254e-02, -1.62861422e-01,  7.35847875e-02,\n",
       "       -6.44739419e-02,  6.03181124e-02,  8.41615200e-02,  9.02965367e-02,\n",
       "       -7.13159442e-02,  2.08856408e-02, -1.35693103e-01, -8.34387317e-02,\n",
       "       -1.33669227e-01,  1.21916093e-01,  1.84632152e-01, -1.17865160e-01,\n",
       "       -3.97908986e-01, -6.37582839e-02, -3.62761587e-01, -9.20062512e-02,\n",
       "       -1.98154092e-01, -1.21161141e-01,  4.53438684e-02, -4.16706726e-02,\n",
       "       -9.25682634e-02, -2.51190633e-01,  1.90842152e-01, -9.84810367e-02,\n",
       "       -3.73466164e-02,  1.52223483e-01,  1.03666104e-01, -1.35434002e-01,\n",
       "        1.91635713e-02, -2.80295368e-02,  4.36545275e-02, -6.50647003e-03,\n",
       "       -8.68569463e-02,  1.32288009e-01,  6.17309622e-02, -5.74537143e-02,\n",
       "        2.23477364e-01, -1.37796402e-01,  6.74511045e-02, -7.64974430e-02,\n",
       "       -2.26761863e-01,  1.42451867e-01,  3.04297712e-02,  1.25355572e-01,\n",
       "        1.99635178e-02, -1.01627886e-01, -4.88474071e-02, -9.74937826e-02,\n",
       "       -3.39901298e-01, -1.90254182e-01, -1.73891023e-01, -1.01652876e-01,\n",
       "        3.29380631e-01,  9.75989029e-02, -5.80702908e-02,  1.63599253e-02,\n",
       "        2.40258902e-01, -1.75971985e-01, -1.05988666e-01,  1.60449773e-01,\n",
       "        3.76871973e-02, -3.39661390e-01, -2.04244047e-01, -2.57811010e-01,\n",
       "       -1.57794863e-01,  1.78446826e-02, -6.28765374e-02, -2.37963796e-01,\n",
       "       -2.90262461e-01, -1.15774274e-02,  1.39353827e-01, -3.05586040e-01,\n",
       "        2.64224857e-01,  1.80097297e-01, -7.38849491e-02,  1.68107688e-01,\n",
       "       -1.46572575e-01, -1.00064680e-01,  1.25209734e-01, -6.10247850e-02,\n",
       "        1.42599657e-01, -1.06952205e-01, -2.14254022e-01, -9.07914620e-03,\n",
       "       -6.92449370e-03,  2.51952466e-02, -1.64003983e-01,  6.80746883e-02,\n",
       "        1.55186728e-01, -1.42946646e-01,  1.73069481e-02, -5.79648605e-03,\n",
       "        5.15284529e-03,  1.27165586e-01, -2.48742819e-01, -6.69192299e-02,\n",
       "        2.20016927e-01, -7.51211494e-02,  2.13564076e-02,  3.48673165e-01,\n",
       "       -5.81951216e-02, -8.89999121e-02, -1.56511590e-01,  2.52397150e-01,\n",
       "       -2.04492714e-02,  2.31753632e-01,  3.33782047e-01, -1.00066923e-01,\n",
       "       -1.43792480e-02,  1.90306753e-01, -2.13991962e-02,  1.75101161e-02,\n",
       "        1.46905914e-01,  9.51553434e-02, -2.69494116e-01, -2.75924683e-01,\n",
       "       -3.48588943e-01,  1.04005657e-01,  1.60008758e-01, -1.74311191e-01,\n",
       "        1.72385246e-01,  1.37426987e-01, -3.08452616e-03,  4.10986356e-02,\n",
       "        2.50359476e-01, -1.38830543e-01, -1.01051018e-01, -3.27078514e-02,\n",
       "        6.56512305e-02,  3.54793340e-01,  1.38222188e-01, -1.08092681e-01,\n",
       "        3.99659947e-02, -2.58323848e-01, -1.47621306e-02,  1.06845103e-01,\n",
       "        1.17141165e-01, -2.51076996e-01,  1.15051270e-01,  1.33106306e-01,\n",
       "       -2.53528774e-01, -1.57530978e-01, -1.52284145e-01, -3.85973901e-02,\n",
       "       -1.61608145e-01, -9.51233953e-02,  4.56629023e-02, -3.10599238e-01,\n",
       "       -2.55535170e-03,  2.15873763e-01, -8.37611780e-02,  1.75826624e-01,\n",
       "       -1.05619930e-01,  3.70748997e-01, -1.24416135e-01,  1.09951971e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# • AraVec Used Here:\n",
    "try:\n",
    "    #Loading the Model Using Word2Vec:\n",
    "    word2vec_model = Word2Vec.load(\n",
    "        r'C:\\Users\\BKSH\\OneDrive - Hashemite University\\Desktop\\BKSH HU\\Natural Language Processing\\Assignments\\Assignment 4\\Models\\Twt-SG'\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading model:\", e)\n",
    "\n",
    "def average_word2vec(tokens, model):\n",
    "    #Filtering Out Words Not in the Vocabulary:\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "#Applying the Function to Get Sentence Embeddings for Each Row:\n",
    "data['sentence_embeddings'] = data['normalized_TWEET'].apply(\n",
    "    lambda x: average_word2vec(x.split(), word2vec_model)\n",
    ")\n",
    "\n",
    "# • Example: ○ Accessing the Vector for the First Sentence:\n",
    "data['sentence_embeddings'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc68ddc-54f8-4ea5-ad56-fa3e852cc31d",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96cd24-2d6a-432b-8cfc-55ef136347cd",
   "metadata": {},
   "source": [
    "# • Train-Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3042bf64-9f54-4589-908d-3fb1a989d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting Word2Vec embeddings to a NumPy array\n",
    "X_word2vec = np.vstack(data['sentence_embeddings'].values)\n",
    "\n",
    "#Target label\n",
    "y = data['LABEL']\n",
    "\n",
    "#Splitting for BoW\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(BOW_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Splitting for TF-IDF\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(TFIDF_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Splitting for Word2Vec\n",
    "X_train_w2v, X_test_w2v, _, _ = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40040883-c9ec-4ec5-b816-967a0c61f2ff",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41452088-3b49-4f45-8da0-969bcaa6b013",
   "metadata": {},
   "source": [
    "# • Train and Evaluate Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "88df758e-f412-4967-aa85-90eea35cf9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes ---\n",
      "BoW Accuracy: 0.6229508196721312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.67      0.62      0.65       276\n",
      "        fear       0.77      0.89      0.83       259\n",
      "         joy       0.61      0.34      0.44       268\n",
      "        love       0.63      0.72      0.67       250\n",
      "        none       0.53      0.89      0.67       307\n",
      "     sadness       0.48      0.24      0.32       258\n",
      "    surprise       0.54      0.40      0.46       201\n",
      "    sympathy       0.72      0.86      0.78       194\n",
      "\n",
      "    accuracy                           0.62      2013\n",
      "   macro avg       0.62      0.62      0.60      2013\n",
      "weighted avg       0.62      0.62      0.60      2013\n",
      "\n",
      "TF-IDF Accuracy: 0.6105315449577745\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.67      0.65       276\n",
      "        fear       0.78      0.86      0.82       259\n",
      "         joy       0.58      0.32      0.41       268\n",
      "        love       0.63      0.68      0.65       250\n",
      "        none       0.46      0.90      0.61       307\n",
      "     sadness       0.52      0.24      0.33       258\n",
      "    surprise       0.70      0.32      0.44       201\n",
      "    sympathy       0.78      0.85      0.81       194\n",
      "\n",
      "    accuracy                           0.61      2013\n",
      "   macro avg       0.64      0.60      0.59      2013\n",
      "weighted avg       0.62      0.61      0.59      2013\n",
      "\n",
      "Skipping Word2Vec for Naive Bayes due to negative values.\n",
      "--- SVM (RBF Kernel) ---\n",
      "BoW Accuracy: 0.6204669647292598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.52      0.72      0.61       276\n",
      "        fear       0.99      0.83      0.90       259\n",
      "         joy       0.42      0.46      0.44       268\n",
      "        love       0.69      0.56      0.62       250\n",
      "        none       0.58      0.96      0.72       307\n",
      "     sadness       0.45      0.24      0.32       258\n",
      "    surprise       0.72      0.30      0.42       201\n",
      "    sympathy       0.84      0.80      0.82       194\n",
      "\n",
      "    accuracy                           0.62      2013\n",
      "   macro avg       0.65      0.61      0.61      2013\n",
      "weighted avg       0.64      0.62      0.61      2013\n",
      "\n",
      "TF-IDF Accuracy: 0.6443119721808246\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.51      0.76      0.61       276\n",
      "        fear       0.99      0.84      0.91       259\n",
      "         joy       0.54      0.41      0.46       268\n",
      "        love       0.71      0.61      0.66       250\n",
      "        none       0.60      0.94      0.73       307\n",
      "     sadness       0.45      0.35      0.40       258\n",
      "    surprise       0.73      0.35      0.47       201\n",
      "    sympathy       0.87      0.81      0.84       194\n",
      "\n",
      "    accuracy                           0.64      2013\n",
      "   macro avg       0.67      0.63      0.63      2013\n",
      "weighted avg       0.66      0.64      0.63      2013\n",
      "\n",
      "Word2Vec Accuracy: 0.6830601092896175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.68      0.75      0.71       276\n",
      "        fear       0.94      0.84      0.89       259\n",
      "         joy       0.57      0.54      0.55       268\n",
      "        love       0.75      0.68      0.71       250\n",
      "        none       0.61      0.92      0.73       307\n",
      "     sadness       0.58      0.43      0.50       258\n",
      "    surprise       0.56      0.39      0.46       201\n",
      "    sympathy       0.81      0.84      0.82       194\n",
      "\n",
      "    accuracy                           0.68      2013\n",
      "   macro avg       0.69      0.67      0.67      2013\n",
      "weighted avg       0.68      0.68      0.68      2013\n",
      "\n",
      "--- Decision Tree ---\n",
      "BoW Accuracy: 0.5638350720317934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.52      0.53      0.52       276\n",
      "        fear       0.95      0.89      0.92       259\n",
      "         joy       0.35      0.50      0.41       268\n",
      "        love       0.66      0.55      0.60       250\n",
      "        none       0.61      0.64      0.62       307\n",
      "     sadness       0.38      0.27      0.31       258\n",
      "    surprise       0.32      0.35      0.34       201\n",
      "    sympathy       0.85      0.79      0.82       194\n",
      "\n",
      "    accuracy                           0.56      2013\n",
      "   macro avg       0.58      0.56      0.57      2013\n",
      "weighted avg       0.58      0.56      0.57      2013\n",
      "\n",
      "TF-IDF Accuracy: 0.5454545454545454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.45      0.49      0.47       276\n",
      "        fear       0.91      0.87      0.89       259\n",
      "         joy       0.34      0.42      0.38       268\n",
      "        love       0.63      0.51      0.57       250\n",
      "        none       0.60      0.68      0.64       307\n",
      "     sadness       0.32      0.26      0.28       258\n",
      "    surprise       0.40      0.37      0.38       201\n",
      "    sympathy       0.78      0.76      0.77       194\n",
      "\n",
      "    accuracy                           0.55      2013\n",
      "   macro avg       0.55      0.54      0.55      2013\n",
      "weighted avg       0.55      0.55      0.55      2013\n",
      "\n",
      "Word2Vec Accuracy: 0.3730750124192747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.37      0.42      0.40       276\n",
      "        fear       0.55      0.50      0.52       259\n",
      "         joy       0.23      0.24      0.23       268\n",
      "        love       0.50      0.44      0.47       250\n",
      "        none       0.46      0.48      0.47       307\n",
      "     sadness       0.23      0.23      0.23       258\n",
      "    surprise       0.21      0.21      0.21       201\n",
      "    sympathy       0.42      0.43      0.43       194\n",
      "\n",
      "    accuracy                           0.37      2013\n",
      "   macro avg       0.37      0.37      0.37      2013\n",
      "weighted avg       0.38      0.37      0.37      2013\n",
      "\n",
      "--- Random Forest ---\n",
      "BoW Accuracy: 0.6140089418777943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.51      0.57       276\n",
      "        fear       0.99      0.89      0.93       259\n",
      "         joy       0.39      0.47      0.43       268\n",
      "        love       0.72      0.53      0.61       250\n",
      "        none       0.62      0.87      0.72       307\n",
      "     sadness       0.40      0.31      0.35       258\n",
      "    surprise       0.43      0.46      0.44       201\n",
      "    sympathy       0.83      0.86      0.85       194\n",
      "\n",
      "    accuracy                           0.61      2013\n",
      "   macro avg       0.63      0.61      0.61      2013\n",
      "weighted avg       0.62      0.61      0.61      2013\n",
      "\n",
      "TF-IDF Accuracy: 0.6284153005464481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.57      0.61      0.59       276\n",
      "        fear       0.98      0.88      0.93       259\n",
      "         joy       0.43      0.45      0.44       268\n",
      "        love       0.68      0.58      0.62       250\n",
      "        none       0.59      0.93      0.72       307\n",
      "     sadness       0.44      0.26      0.32       258\n",
      "    surprise       0.56      0.39      0.46       201\n",
      "    sympathy       0.78      0.88      0.83       194\n",
      "\n",
      "    accuracy                           0.63      2013\n",
      "   macro avg       0.63      0.62      0.62      2013\n",
      "weighted avg       0.62      0.63      0.61      2013\n",
      "\n",
      "Word2Vec Accuracy: 0.5842026825633383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.55      0.73      0.62       276\n",
      "        fear       0.72      0.75      0.74       259\n",
      "         joy       0.54      0.35      0.43       268\n",
      "        love       0.59      0.72      0.65       250\n",
      "        none       0.55      0.87      0.68       307\n",
      "     sadness       0.45      0.22      0.30       258\n",
      "    surprise       0.43      0.20      0.27       201\n",
      "    sympathy       0.73      0.73      0.73       194\n",
      "\n",
      "    accuracy                           0.58      2013\n",
      "   macro avg       0.57      0.57      0.55      2013\n",
      "weighted avg       0.57      0.58      0.56      2013\n",
      "\n",
      "--- AdaBoost ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BKSH\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Accuracy: 0.41033283656234476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.24      0.73      0.36       276\n",
      "        fear       0.99      0.87      0.92       259\n",
      "         joy       0.15      0.20      0.17       268\n",
      "        love       0.85      0.28      0.43       250\n",
      "        none       0.47      0.26      0.33       307\n",
      "     sadness       0.00      0.00      0.00       258\n",
      "    surprise       0.57      0.20      0.30       201\n",
      "    sympathy       0.64      0.79      0.71       194\n",
      "\n",
      "    accuracy                           0.41      2013\n",
      "   macro avg       0.49      0.42      0.40      2013\n",
      "weighted avg       0.47      0.41      0.39      2013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BKSH\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.40735221063089916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.24      0.76      0.37       276\n",
      "        fear       0.99      0.85      0.92       259\n",
      "         joy       0.15      0.22      0.18       268\n",
      "        love       0.80      0.32      0.45       250\n",
      "        none       0.48      0.21      0.29       307\n",
      "     sadness       0.14      0.00      0.01       258\n",
      "    surprise       0.65      0.15      0.24       201\n",
      "    sympathy       0.64      0.80      0.71       194\n",
      "\n",
      "    accuracy                           0.41      2013\n",
      "   macro avg       0.51      0.41      0.40      2013\n",
      "weighted avg       0.50      0.41      0.39      2013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BKSH\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Accuracy: 0.4972677595628415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.54      0.53      0.53       276\n",
      "        fear       0.63      0.68      0.65       259\n",
      "         joy       0.40      0.21      0.28       268\n",
      "        love       0.50      0.64      0.56       250\n",
      "        none       0.53      0.79      0.64       307\n",
      "     sadness       0.29      0.15      0.20       258\n",
      "    surprise       0.36      0.27      0.31       201\n",
      "    sympathy       0.48      0.65      0.55       194\n",
      "\n",
      "    accuracy                           0.50      2013\n",
      "   macro avg       0.47      0.49      0.47      2013\n",
      "weighted avg       0.47      0.50      0.47      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initializing the classifiers\n",
    "classifiers = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM (RBF Kernel)\": SVC(kernel='rbf', probability=True),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "#Evaluating each classifier on all three representations\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    \n",
    "    #BoW\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    y_pred_bow = clf.predict(X_test_bow)\n",
    "    print(\"BoW Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "    print(classification_report(y_test, y_pred_bow))\n",
    "    \n",
    "    #TF-IDF\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    y_pred_tfidf = clf.predict(X_test_tfidf)\n",
    "    print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "    print(classification_report(y_test, y_pred_tfidf))\n",
    "    \n",
    "    #Skipping Word2Vec for MultinomialNB ==> due to its inability to process negative values.\n",
    "    if name == \"Naive Bayes\":\n",
    "        print(\"Skipping Word2Vec for Naive Bayes due to negative values.\")\n",
    "        continue\n",
    "    \n",
    "    #Word2Vec\n",
    "    clf.fit(X_train_w2v, y_train)\n",
    "    y_pred_w2v = clf.predict(X_test_w2v)\n",
    "    print(\"Word2Vec Accuracy:\", accuracy_score(y_test, y_pred_w2v))\n",
    "    print(classification_report(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4b568-1287-41e2-bc51-7f6b5f09c6a9",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b793620-6a53-40f1-be95-cec45a8a4fff",
   "metadata": {},
   "source": [
    "# • Encode the Labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c98f49c0-d514-4cbc-bee9-5ff28dbdb186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the target labels\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "#Converting to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_encoded)\n",
    "y_test_onehot = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c3cab-136b-498b-9e5e-5ac821e7db82",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd832461-3ce0-40f5-adaa-a5ca1c7f79c0",
   "metadata": {},
   "source": [
    "# • Define the FNN Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0242675-1393-4bc4-93fb-c8355431f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fnn_model(input_dim, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.3),  \n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')  \n",
    "    ])\n",
    "    \n",
    "    #Compiling the model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075fe2c7-a822-44ca-9a52-e51b0801a64c",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbe4bc-3554-4b4c-a23e-2711d096e433",
   "metadata": {},
   "source": [
    "# • Train the FNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "030292cf-d785-4805-a2fa-d7e4c154a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BKSH\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.2598 - loss: 1.9580 - val_accuracy: 0.5941 - val_loss: 1.2452\n",
      "Epoch 2/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6696 - loss: 1.0131 - val_accuracy: 0.6379 - val_loss: 1.0889\n",
      "Epoch 3/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8062 - loss: 0.6279 - val_accuracy: 0.6329 - val_loss: 1.1440\n",
      "Epoch 4/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8858 - loss: 0.4123 - val_accuracy: 0.6289 - val_loss: 1.2572\n",
      "Epoch 5/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9154 - loss: 0.2896 - val_accuracy: 0.6150 - val_loss: 1.4120\n",
      "Epoch 6/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9458 - loss: 0.2054 - val_accuracy: 0.6051 - val_loss: 1.5474\n",
      "Epoch 7/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9522 - loss: 0.1637 - val_accuracy: 0.6036 - val_loss: 1.6980\n",
      "Epoch 8/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9652 - loss: 0.1272 - val_accuracy: 0.5946 - val_loss: 1.8408\n",
      "Epoch 9/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9715 - loss: 0.1143 - val_accuracy: 0.6001 - val_loss: 1.9391\n",
      "Epoch 10/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9718 - loss: 0.0992 - val_accuracy: 0.5986 - val_loss: 2.0647\n"
     ]
    }
   ],
   "source": [
    "# • Training with TF-IDF:\n",
    "\n",
    "#Creating the model for TF-IDF input\n",
    "tfidf_model = create_fnn_model(input_dim=X_train_tfidf.shape[1], num_classes=y_train_onehot.shape[1])\n",
    "\n",
    "#Training the model\n",
    "history_tfidf = tfidf_model.fit(\n",
    "    X_train_tfidf, y_train_onehot,\n",
    "    validation_data=(X_test_tfidf, y_test_onehot),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b04abc57-9486-430a-80ee-19d739066dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BKSH\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.3865 - loss: 1.7210 - val_accuracy: 0.6066 - val_loss: 1.1383\n",
      "Epoch 2/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6092 - loss: 1.1668 - val_accuracy: 0.6488 - val_loss: 1.0364\n",
      "Epoch 3/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6509 - loss: 1.0398 - val_accuracy: 0.6622 - val_loss: 0.9903\n",
      "Epoch 4/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6623 - loss: 0.9955 - val_accuracy: 0.6602 - val_loss: 0.9858\n",
      "Epoch 5/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6873 - loss: 0.9243 - val_accuracy: 0.6721 - val_loss: 0.9572\n",
      "Epoch 6/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6956 - loss: 0.8927 - val_accuracy: 0.6662 - val_loss: 0.9598\n",
      "Epoch 7/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7018 - loss: 0.8708 - val_accuracy: 0.6796 - val_loss: 0.9497\n",
      "Epoch 8/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7206 - loss: 0.8308 - val_accuracy: 0.6662 - val_loss: 0.9600\n",
      "Epoch 9/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7190 - loss: 0.8203 - val_accuracy: 0.6741 - val_loss: 0.9661\n",
      "Epoch 10/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7369 - loss: 0.7693 - val_accuracy: 0.6741 - val_loss: 0.9589\n"
     ]
    }
   ],
   "source": [
    "# • Training with Word2Vec:\n",
    "\n",
    "#Creating the model for Word2Vec input\n",
    "w2v_model = create_fnn_model(input_dim=X_train_w2v.shape[1], num_classes=y_train_onehot.shape[1])\n",
    "\n",
    "#Training the model\n",
    "history_w2v = w2v_model.fit(\n",
    "    X_train_w2v, y_train_onehot,\n",
    "    validation_data=(X_test_w2v, y_test_onehot),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56aa6b-e0c8-47b2-9e6f-919420ad5a1e",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb1314-8c7b-4380-9c34-daa0e3f86415",
   "metadata": {},
   "source": [
    "# • Evaluate the Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df5676ae-f697-42be-9f4c-aaa8b0c2c8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5979 - loss: 2.0533 \n",
      "TF-IDF Test Accuracy: 59.86%\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6836 - loss: 0.9471 \n",
      "Word2Vec Test Accuracy: 67.41%\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "FNN (TF-IDF) Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.60      0.58       276\n",
      "           1       0.93      0.86      0.89       259\n",
      "           2       0.43      0.50      0.46       268\n",
      "           3       0.63      0.59      0.61       250\n",
      "           4       0.58      0.66      0.62       307\n",
      "           5       0.39      0.36      0.37       258\n",
      "           6       0.51      0.38      0.43       201\n",
      "           7       0.83      0.85      0.84       194\n",
      "\n",
      "    accuracy                           0.60      2013\n",
      "   macro avg       0.61      0.60      0.60      2013\n",
      "weighted avg       0.60      0.60      0.60      2013\n",
      "\n",
      "FNN (Word2Vec) Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70       276\n",
      "           1       0.90      0.84      0.87       259\n",
      "           2       0.61      0.51      0.56       268\n",
      "           3       0.70      0.72      0.71       250\n",
      "           4       0.64      0.83      0.72       307\n",
      "           5       0.55      0.42      0.48       258\n",
      "           6       0.52      0.42      0.46       201\n",
      "           7       0.80      0.85      0.82       194\n",
      "\n",
      "    accuracy                           0.67      2013\n",
      "   macro avg       0.67      0.67      0.66      2013\n",
      "weighted avg       0.67      0.67      0.67      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# • Evaluating on Test Data:\n",
    "\n",
    "#TF-IDF\n",
    "tfidf_loss, tfidf_accuracy = tfidf_model.evaluate(X_test_tfidf, y_test_onehot)\n",
    "print(f\"TF-IDF Test Accuracy: {tfidf_accuracy * 100:.2f}%\")\n",
    "\n",
    "#Word2Vec\n",
    "w2v_loss, w2v_accuracy = w2v_model.evaluate(X_test_w2v, y_test_onehot)\n",
    "print(f\"Word2Vec Test Accuracy: {w2v_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "#Predicting the probabilities for the TF-IDF model\n",
    "y_pred_tfidf = tfidf_model.predict(X_test_tfidf)\n",
    "\n",
    "#Converting the probabilities to class labels\n",
    "y_pred_tfidf_labels = y_pred_tfidf.argmax(axis=1)\n",
    "\n",
    "#Converting one-hot test labels to class labels\n",
    "y_test_labels = y_test_onehot.argmax(axis=1)\n",
    "\n",
    "#Predicting the probabilities for the Word2Vec model\n",
    "y_pred_w2v = w2v_model.predict(X_test_w2v)\n",
    "\n",
    "#Converting the probabilities to class labels\n",
    "y_pred_w2v_labels = y_pred_w2v.argmax(axis=1)\n",
    "\n",
    "print(\"FNN (TF-IDF) Classification Report\")\n",
    "print(classification_report(y_test_labels, y_pred_tfidf_labels))\n",
    "\n",
    "print(\"FNN (Word2Vec) Classification Report\")\n",
    "print(classification_report(y_test_labels, y_pred_w2v_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661967e0-3168-499b-b886-5d44de6d4ea7",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4151a54-bca2-4a8b-b6bf-ad33558044b4",
   "metadata": {},
   "source": [
    "# • Tokenize the Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d13e1587-2b7c-4486-be13-81d2cca9cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 33671\n"
     ]
    }
   ],
   "source": [
    "#Defining the tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")  \n",
    "tokenizer.fit_on_texts(data['normalized_TWEET'])\n",
    "\n",
    "#Converting the text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(data['normalized_TWEET'])\n",
    "\n",
    "#Getting the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4317dfd-2f63-4f2b-9a5b-a463e7ffe466",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862575ec-44fa-4721-b86a-0d1f4a8bab38",
   "metadata": {},
   "source": [
    "# • Pad the Sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b07f7ab3-89e1-4ffa-827d-7a58042faf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the maximum sequence length\n",
    "max_length = 100  \n",
    "\n",
    "#Padding the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373eaf50-7e86-4037-a6b9-ea849c5ea1d2",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1a975-bab1-41cc-9a30-5cccbecdd646",
   "metadata": {},
   "source": [
    "# • Encode the Labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0b7b0eb-5ca6-46ef-93cb-14f75b9bc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(data['LABEL'])\n",
    "\n",
    "#Converting to one-hot encoding\n",
    "y_onehot = to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d1729e-40fd-48fd-8010-e9bc3ab9e720",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba454e4-e165-40e4-a177-e832316f07bb",
   "metadata": {},
   "source": [
    "# • Train-Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0dea8fd4-52a6-480e-b0a6-877a59a277a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sequences, y_onehot, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84103a6-3462-40aa-be83-5e96303f9e6f",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a274e31-8562-4323-a2ec-730d24167343",
   "metadata": {},
   "source": [
    "# • Load Pre-trained Word2Vec Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f6a26abe-329a-4be2-b461-cbd6e9c31f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BKSH\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# • Creating an Embedding Matrix:\n",
    "\n",
    "#Initializing the embedding matrix\n",
    "embedding_dim = word2vec_model.vector_size  # Dimension of Word2Vec embeddings\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "#Populating the embedding matrix\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "#Defining an Embedding Layer:\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_length,\n",
    "    trainable=False  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987b947-6567-441b-8c61-b66d36068f85",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9849f-05e8-439b-8d79-03897ed8c3a1",
   "metadata": {},
   "source": [
    "# • Define the LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "719c0026-6a61-4c01-835b-75999d4d1d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the LSTM model\n",
    "def create_lstm_model():\n",
    "    model = Sequential([\n",
    "        embedding_layer,  # Pre-trained Word2Vec embeddings\n",
    "        Bidirectional(LSTM(128, return_sequences=False)),  # BiLSTM layer\n",
    "        Dropout(0.3),  # Dropout for regularization\n",
    "        Dense(64, activation='relu'),  # Fully connected layer\n",
    "        Dropout(0.3),\n",
    "        Dense(y_train.shape[1], activation='softmax')  # Output layer\n",
    "    ])\n",
    "    \n",
    "    #Compiling the model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ea9ac-d957-42a2-b714-372d840eb249",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab0f21-0aa2-45b6-a8d9-a8bef8b90b9c",
   "metadata": {},
   "source": [
    "# • Train the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "49024e4a-0956-4cc2-8ef3-0c37bd9f7614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 71ms/step - accuracy: 0.4358 - loss: 1.5953 - val_accuracy: 0.6503 - val_loss: 1.0060\n",
      "Epoch 2/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 70ms/step - accuracy: 0.6656 - loss: 1.0091 - val_accuracy: 0.6647 - val_loss: 0.9510\n",
      "Epoch 3/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 64ms/step - accuracy: 0.6949 - loss: 0.9021 - val_accuracy: 0.6841 - val_loss: 0.9028\n",
      "Epoch 4/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 75ms/step - accuracy: 0.7360 - loss: 0.7813 - val_accuracy: 0.6791 - val_loss: 0.9012\n",
      "Epoch 5/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 74ms/step - accuracy: 0.7387 - loss: 0.7560 - val_accuracy: 0.6965 - val_loss: 0.8956\n",
      "Epoch 6/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 70ms/step - accuracy: 0.7762 - loss: 0.6578 - val_accuracy: 0.6890 - val_loss: 0.9191\n",
      "Epoch 7/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - accuracy: 0.7931 - loss: 0.6067 - val_accuracy: 0.6850 - val_loss: 0.9062\n",
      "Epoch 8/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.7989 - loss: 0.5710 - val_accuracy: 0.6980 - val_loss: 0.9329\n",
      "Epoch 9/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 66ms/step - accuracy: 0.8410 - loss: 0.4754 - val_accuracy: 0.6985 - val_loss: 0.9484\n",
      "Epoch 10/10\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 72ms/step - accuracy: 0.8518 - loss: 0.4379 - val_accuracy: 0.6756 - val_loss: 1.0957\n"
     ]
    }
   ],
   "source": [
    "#Creating the LSTM model\n",
    "lstm_model = create_lstm_model()\n",
    "\n",
    "#Training the model\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49195b9e-1e5c-4443-a2fe-f5cfb467ea07",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b6562-6deb-4e47-b483-011d1486ebfc",
   "metadata": {},
   "source": [
    "# • Evaluate the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "410d0c44-426a-42de-9356-d7cb378d0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.6854 - loss: 1.0653\n",
      "LSTM Test Accuracy: 67.56%\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step  \n",
      "LSTM Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.66      0.71       276\n",
      "           1       0.94      0.93      0.93       259\n",
      "           2       0.57      0.55      0.56       268\n",
      "           3       0.79      0.57      0.66       250\n",
      "           4       0.64      0.84      0.73       307\n",
      "           5       0.53      0.45      0.49       258\n",
      "           6       0.41      0.59      0.49       201\n",
      "           7       0.92      0.80      0.85       194\n",
      "\n",
      "    accuracy                           0.68      2013\n",
      "   macro avg       0.70      0.67      0.68      2013\n",
      "weighted avg       0.69      0.68      0.68      2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# • Evaluating on Test Data:\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"LSTM Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "#LSTM Model Predictions\n",
    "y_pred_lstm = lstm_model.predict(X_test)\n",
    "y_pred_lstm_labels = y_pred_lstm.argmax(axis=1)\n",
    "y_test_lstm_labels = y_test.argmax(axis=1)\n",
    "\n",
    "print(\"LSTM Classification Report\")\n",
    "print(classification_report(y_test_lstm_labels, y_pred_lstm_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44fb0c-a5bb-4ad9-bf11-04d50bfa077d",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460849a-ea40-4b16-9e18-0e6b3fb70424",
   "metadata": {},
   "source": [
    "# <center> **The End** </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bb31a-0252-4859-b31e-6d5896a476b8",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
